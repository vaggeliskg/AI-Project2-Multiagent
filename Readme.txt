evaluationFunction: Η συνάρτηση αυτή αποτίμησης αρχικά επιστρέφει +άπειρο και -άπειρο για τερματικές 
καταστάσεις στις οποίες κερδίζει και χάνει αντίστοιχα. Χρησιμοποιεί αποστάσεις manhattan για να βρει το πιο
κοντινό φαγητό και το πιο κοντινό φάντασμα καθώς αυτά είναι που τον επηρεάζουν άμεσα. Προστίθεται λοιπόν 
στο σκορ +1/closest_food ώστε να επιλέγει πρώτα το φαγητό που είναι πιο κοντά σε αυτόν ενώ αφαρείται 
100/closest_ghost όταν το φάντασμα βρίσκεται σε πολύ μικρή απόσταση από αυτόν ώστε να το αποφεύγει, οι 
τιμές αυτές που δίνονται στο σκορ προέκυψαν μετά από δοκιμές και το πώς αντιδρούσε ο pacman και φάνηκε 
οι συγκρεκριμένες να είναι από τις πιο ικανοποιητικές. Επίσης στο τέλος επιστρέφεται το άθροισμα 
score + successorGameState.getScore() αφού η getScore παρέχει ένα επιπλέον σκορ που βελτιώνει κατά πολύ
την evaluationFunction.

MinimaxAgent: Σε αυτή την κλάση υλοποιήθηκε ο αλγόριθμος minimax των διαφανειών. Υλοποιήθηκαν 2 συναρτήσεις
μία για όταν είναι σειρά του pacman και μία όταν είναι η σειρά κάποιου ghost. Συγκεκριμένα, αρχικά ελέγχεται αν
είμαστε σε τερματική κατάσταση δηλαδή νική,ήττα ή φύλλο οπότε επιστρέφεται self.evaluationFunction σαν tuple διότι
αργότερα χρειαζόμαστε (value,action). Υπάρχει η λούπα που εξετάζει κάθε action και καλεί την αντίστοιχη
συνάρτηση δηλαδή min_val αν μετά είναι σειρά ghost ή max_val αν είναι σειρά pacman,έπειτα ελέγχεται αναδρομικά 
όλο το δέντρο και εξετάζεται αν η τιμή που επιστρέφεται είναι > ή < m για κάθε συνάρτηση ώστε να γίνει η σωστή επιλογή
και επιστρέφεται και το action που οδηγεί στην επιλογή αυτή. Η μόνη διαφορά με τον αλγόριθμο των διαφανειών είναι ότι
εδώ μπορεί μετά από την κίνηση ενός φαντάσματος να παίζει είτε πάλι φάντασμα οπότε το depth δεν αλλάζει αλλά μόνο
το index του agent είτε ο πακμαν,οπότε το depth εδώ αυξάνεται αλλά καλείται και διαφορετική συνάρτηση. Ο πιο εύκολος
τρόπος που βρήκα για να ελέγχεται ποιος παίζει μετά είναι με τη χρήση της μεταβλητής next_agent. Αν ο επόμενος είναι
ο πάκμαν τότε σίγουρα πρέπει agentIndex == gameState.getNumAgents() - 1, έτσι αυτή η συνθήκη θα ισχυεί μόνο όταν αυτός 
που παίζει τώρα είναι το τελευταίο φάντασμα, δηλαδή ο επόμενος θα είναι ο πάκμαν άρα πρέπει να αυξηθεί το βάθος και να
κληθεί η αντίστοιχη συνάρτηση.

AlphaBetaAgent: Ο αλγόριθμος είναι ίδιος με πριν με την προσθήκη των a,b ώστε να γίνεται το κλάδεμα σωστά.
Ο αλγόριθμος για το πως να υλοποιηθούν δινόταν στην εκφώνηση οπότε δεν υπήρξε κάποια ιδιαίτερη δυσκολία 
αφού βασιζόταν πλήρως στο προηγούμενο ερώτημα.

ExpectimaxAgent: Ο αλγόριθμος αυτός επίσης βασίζεται στο 2ο ερώτημα με τη μόνη διαφορά ότι τώρα τα φαντάσματα
δεν κάνουν την min επιλογή αλλά επιλέγουν λαμβάνοντας υπόψη τους και την πιθανότητα ενός action. Αυτό γίνεται 
υπολογίζοντας την πιθανότητα του κάθε action και χρησιμοποιώντας τον γνωστό τύπο value += first_element * probability, 
όπου first_element η τιμή που επιστρέφεται από στα φύλλα (self.evaluationFunction), για τους παραπάνω κόμβους
είναι το value.

betterEvaluationFunction: Παρόμοια και με το πρώτο ερώτημα τα βάρη της συνάρτησης αυτής προέκυψαν μετά από
δοκιμές που βασίστηκαν πάνω στο q1. Η διαφορές τώρα είναι χωρίς την πρόσβαση στο actions χρησιμοποιήθηκαν 
currentGameState και ότι πλέον ο pacman χρησιμοποιεί manhattan distances για να βρει την απόσταση στο πιο κοντινό 
φαγητό και στο πιο κοντινό φάντασμα και στο τέλος ενημερώνει το σκορ που βασίζεται στα αποτελέσματα των παραπάνω αποστάσεων.
Τέλος προστίθεται και το successorGameState.getScore() που βελτιώνει περαιτέρω τα αποτελέσματα της συνάρτησης κάνοντας την να
λειτουργεί αποτελεσματικά και σχετικά γρήγορα.